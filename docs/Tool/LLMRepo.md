# Very Useful Repo about LLM

ç°åœ¨çš„æ·±åº¦å­¦ä¹ å¤§æ¨¡å‹æœ‰å¾ˆå¤šç°æˆå¯ç”¨çš„å·¥å…·è¦å­¦ä¼šåˆ©ç”¨å·²æœ‰çš„å·¥å…·ï¼Œè¿™æ ·èƒ½æé«˜æ•ˆç‡ï¼Œä¹Ÿæ–¹ä¾¿èƒ½ä»åˆ«äººçš„ä»£ç é‡Œå­¦ä¹ ã€‚
## [Sentence Transformers](https://www.sbert.net/)

å¯ä»¥å¾ˆæ–¹ä¾¿çš„è·å–embeddingï¼Œé›†æˆäº†å¾ˆå¤šMTEBä¸Šé¢ä¼˜ç§€çš„æ¨¡å‹ï¼Œæ–¹ä¾¿è®­ç»ƒå¾®è°ƒã€‚

> Sentence Transformers (a.k.a. SBERT) is the go-to Python module for accessing, using, and training state-of-the-art text and image embedding models. It can be used to compute embeddings using Sentence Transformer models ([quickstart](https://www.sbert.net/docs/quickstart.html#sentence-transformer)) or to calculate similarity scores using Cross-Encoder models ([quickstart](https://www.sbert.net/docs/quickstart.html#cross-encoder)). This unlocks a wide range of applications, includingÂ [semantic search](https://www.sbert.net/examples/applications/semantic-search/README.html),Â [semantic textual similarity](https://www.sbert.net/docs/usage/semantic_textual_similarity.html), andÂ [paraphrase mining](https://www.sbert.net/examples/applications/paraphrase-mining/README.html).

> A wide selection of overÂ [5,000 pre-trained Sentence Transformers models](https://huggingface.co/models?library=sentence-transformers)Â are available for immediate use on ğŸ¤— Hugging Face, including many of the state-of-the-art models from theÂ [Massive Text Embeddings Benchmark (MTEB) leaderboard](https://huggingface.co/spaces/mteb/leaderboard). Additionally, it is easy toÂ [train or finetune your own models](https://www.sbert.net/docs/sentence_transformer/training_overview.html)Â using Sentence Transformers, enabling you to create custom models for your specific use cases.
> ![[Pasted image 20250224173824.png]]


## [FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding/tree/master)

å®ç°äº†BGEç­‰BAAIè‡ªå·±æå‡ºçš„ä¸€ç³»åˆ—æ¨¡å‹ï¼Œä¹Ÿé›†æˆäº†FinetuneåŠŸèƒ½ï¼Œä½†æ˜¯æ¡†æ¶åªèƒ½åœ¨è‡ªå·±çš„æ¨¡å‹ä¸Šç”¨ï¼Œé€šç”¨å‹æœ‰é™ï¼Œä¸è¿‡ä¹Ÿå¯ä»¥å½“åšä»£ç å‚è€ƒã€‚

> BGE (BAAI General Embedding) focuses on retrieval-augmented LLMs, consisting of the following projects currently:
> ![[Pasted image 20250224165701.png]]



## [LLama Factory](https://github.com/hiyouga/LLaMA-Factory)

ä¸Šé¢é›†æˆæœ‰éå¸¸å¤šçš„æ¨¡å‹å’Œæ•°æ®å’Œå¾®è°ƒæ–¹æ³•ï¼Œå¾®è°ƒä¸äºŒä¹‹é€‰ï¼Œä½†æ˜¯è®­ç»ƒæ•°æ®å±€é™äºQAå’ŒPreferenceç±»å‹çš„æ•°æ®ï¼Œä»»åŠ¡ç±»å‹ä¼šæ¯”è¾ƒå±€é™ã€‚

> Easily fine-tune 100+ large language models with zero-codeÂ [CLI](https://github.com/hiyouga/LLaMA-Factory#quickstart)Â andÂ [Web UI](https://github.com/hiyouga/LLaMA-Factory#fine-tuning-with-llama-board-gui-powered-by-gradio)
> 
> ![[Pasted image 20250224172354.png]]
> 
> Features
> 
> - **Various models**: LLaMA, LLaVA, Mistral, Mixtral-MoE, Qwen, Qwen2-VL, DeepSeek, Yi, Gemma, ChatGLM, Phi, etc.
> - **Integrated methods**: (Continuous) pre-training, (multimodal) supervised fine-tuning, reward modeling, PPO, DPO, KTO, ORPO, etc.
> - **Scalable resources**: 16-bit full-tuning, freeze-tuning, LoRA and 2/3/4/5/6/8-bit QLoRA via AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ.
> - **Advanced algorithms**:Â [GaLore](https://github.com/jiaweizzhao/GaLore),Â [BAdam](https://github.com/Ledzy/BAdam),Â [APOLLO](https://github.com/zhuhanqing/APOLLO),Â [Adam-mini](https://github.com/zyushun/Adam-mini), DoRA, LongLoRA, LLaMA Pro, Mixture-of-Depths, LoRA+, LoftQ and PiSSA.
> - **Practical tricks**:Â [FlashAttention-2](https://github.com/Dao-AILab/flash-attention),Â [Unsloth](https://github.com/unslothai/unsloth),Â [Liger Kernel](https://github.com/linkedin/Liger-Kernel), RoPE scaling, NEFTune and rsLoRA.
> - **Wide tasks**: Multi-turn dialogue, tool using, image understanding, visual grounding, video recognition, audio understanding, etc.
> - **Experiment monitors**: LlamaBoard, TensorBoard, Wandb, MLflow, SwanLab, etc.
> - **Faster inference**: OpenAI-style API, Gradio UI and CLI with vLLM worker.



## [å¼€æºå¤§æ¨¡å‹é£Ÿç”¨æŒ‡å—-self-llm](https://github.com/datawhalechina/self-llm/tree/master)

å¤§æ¨¡å‹ä½¿ç”¨è®­ç»ƒçš„ä¸­æ–‡æŒ‡å—ï¼Œè´¡çŒ®è€…ä¼—å¤šï¼Œæœ‰äº›æ¯”è¾ƒè¯¦ç»†ï¼Œå¯èƒ½ç”±ä¸åŒè´¡çŒ®è€…å†³å®šä¸åŒé£æ ¼

> æœ¬é¡¹ç›®æ˜¯ä¸€ä¸ªå›´ç»•å¼€æºå¤§æ¨¡å‹ã€é’ˆå¯¹å›½å†…åˆå­¦è€…ã€åŸºäº Linux å¹³å°çš„ä¸­å›½å®å®ä¸“å±å¤§æ¨¡å‹æ•™ç¨‹ï¼Œé’ˆå¯¹å„ç±»å¼€æºå¤§æ¨¡å‹æä¾›åŒ…æ‹¬ç¯å¢ƒé…ç½®ã€æœ¬åœ°éƒ¨ç½²ã€é«˜æ•ˆå¾®è°ƒç­‰æŠ€èƒ½åœ¨å†…çš„å…¨æµç¨‹æŒ‡å¯¼ï¼Œç®€åŒ–å¼€æºå¤§æ¨¡å‹çš„éƒ¨ç½²ã€ä½¿ç”¨å’Œåº”ç”¨æµç¨‹ï¼Œè®©æ›´å¤šçš„æ™®é€šå­¦ç”Ÿã€ç ”ç©¶è€…æ›´å¥½åœ°ä½¿ç”¨å¼€æºå¤§æ¨¡å‹ï¼Œå¸®åŠ©å¼€æºã€è‡ªç”±çš„å¤§æ¨¡å‹æ›´å¿«èå…¥åˆ°æ™®é€šå­¦ä¹ è€…çš„ç”Ÿæ´»ä¸­ã€‚

> æœ¬é¡¹ç›®çš„ä¸»è¦å†…å®¹åŒ…æ‹¬ï¼š
> 
> 1. åŸºäº Linux å¹³å°çš„å¼€æº LLM ç¯å¢ƒé…ç½®æŒ‡å—ï¼Œé’ˆå¯¹ä¸åŒæ¨¡å‹è¦æ±‚æä¾›ä¸åŒçš„è¯¦ç»†ç¯å¢ƒé…ç½®æ­¥éª¤
> 2. é’ˆå¯¹å›½å†…å¤–ä¸»æµå¼€æº LLM çš„éƒ¨ç½²ä½¿ç”¨æ•™ç¨‹ï¼ŒåŒ…æ‹¬ LLaMAã€ChatGLMã€InternLM ç­‰ï¼›
> 3. å¼€æº LLM çš„éƒ¨ç½²åº”ç”¨æŒ‡å¯¼ï¼ŒåŒ…æ‹¬å‘½ä»¤è¡Œè°ƒç”¨ã€åœ¨çº¿ Demo éƒ¨ç½²ã€LangChain æ¡†æ¶é›†æˆç­‰ï¼›
> 4. å¼€æº LLM çš„å…¨é‡å¾®è°ƒã€é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬åˆ†å¸ƒå¼å…¨é‡å¾®è°ƒã€LoRAã€ptuning ç­‰ã€‚

## [Phi Cookbook: Hands-On Examples with Microsoft's Phi Models](https://github.com/microsoft/Phi-3CookBook#phi-cookbook-hands-on-examples-with-microsofts-phi-models)

Microsoftçš„Phiçš„ä½¿ç”¨æŒ‡å—ï¼Œæœ‰å¾ˆå¤šæŒ‡å—å¯ä»¥å‚è€ƒ

> Phi, is a family of open AI models developed by Microsoft. Phi models are the most capable and cost-effective small language models (SLMs) available, outperforming models of the same size and next size up across a variety of language, reasoning, coding, and math benchmarks. The Phi-3 Family includes mini, small, medium and vision versions, trained based on different parameter amounts to serve various application scenarios. For more detailed information about Microsoft's Phi family, please visit theÂ [Welcome to the Phi Family](https://github.com/microsoft/Phi-3CookBook/blob/main/md/01.Introduce/Phi3Family.md)Â page.
> ![[Pasted image 20250224173744.png]]