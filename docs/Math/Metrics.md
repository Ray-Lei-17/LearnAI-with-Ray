## Metrics summary
### Perplexity (PPL)

**PPL** 是 **Perplexity**（困惑度）的缩写，是评价语言模型（Language Model, LM）好坏最常用的指标之一。

简单来说，PPL 衡量的是模型在预测下一个词时的“困惑程度”。模型对文本序列的概率分布预测得越准确，它的“困惑”就越小，PPL 值也就越低。

---

#### 1. PPL 的直观理解

你可以把 PPL 理解为模型在面对下一个词时的**平均选择分支数**：

- 如果一个模型的 PPL = 10，说明模型在预测下一个词时，相当于在 10 个等可能的词中做选择。
    
- **结论：** PPL 值越低，说明模型生成的句子越流畅、越符合自然语言逻辑，模型的性能越好。
    

---

#### 2. 如何计算 PPL？

PPL 的计算基于句子出现的概率。假设我们有一个包含 $N$ 个词的句子 $W = (w_1, w_2, ..., w_N)$。

##### 核心公式

PPL 是句子概率倒数的几何平均值，公式如下：

$$PPL(W) = P(w_1, w_2, ..., w_N)^{-\frac{1}{N}} = \sqrt[N]{\frac{1}{P(w_1, w_2, ..., w_N)}}$$

根据链式法则，句子的概率 $P(W)$ 可以拆解为：

$$P(w_1, w_2, ..., w_N) = \prod_{i=1}^{N} P(w_i | w_1, w_2, ..., w_{i-1})$$

##### 与交叉熵（Cross Entropy）的关系

在实际工程（如深度学习训练）中，我们通常不直接计算概率乘积（因为容易溢出），而是通过**交叉熵损失（Cross Entropy Loss）**来计算 PPL。这是最常用的计算方式：

1. 计算平均交叉熵损失 $H(W)$：
    
    $$H(W) = -\frac{1}{N} \sum_{i=1}^{N} \log P(w_i | w_{<i})$$
    
2. 通过指数函数还原：
    
    $$PPL = e^{H(W)} \quad (\text{或 } 2^{H(W)} \text{，取决于对数底数})$$
    

---

#### 3. PPL 的常见数值范围

- **完美预测：** 如果模型每次都能以 100% 的概率猜对下一个词，PPL = 1。
    
- **随机预测：** 如果词表大小为 $V$，模型完全靠盲猜，PPL = $V$（通常是几万甚至十几万）。
    
- **优秀模型：** 在通用的英文语料上（如 Penn Treebank），优秀的模型 PPL 通常在 20 到 60 之间。
    

---

#### 4. 使用 PPL 的注意事项

- **分词影响：** PPL 受分词方式（Tokenizer）影响很大。不同分词方案（如 Word-level vs BPE）产生的 PPL 无法直接比较。
    
- **语料相关：** PPL 只能在相同的测试集上比较才有意义。
    
- **不等于人类感官：** 虽然 PPL 低通常意味着写得好，但有时 PPL 极低的模型可能会陷入“复读机”模式，因此还需要配合 BLEU、ROUGE 或人工评估。
    

**你想了解如何用 Python 或 PyTorch 代码来实现 PPL 的计算吗？**

### Calibration 

在机器学习，尤其是分类任务中，**校准 (Calibration)** 是一个关键的概念，它衡量的是模型对自身预测**置信度**的可靠性。

##### 1. 什么是 Calibration（校准）？

在机器学习中，**校准**指的是模型输出的**预测概率（或置信度）**与实际观察到的**准确率**之间的一致性。

理想的校准状态（Perfect Calibration）：

一个完美校准的分类器，当它以 $P$ 的概率做出预测时，它的预测有正好 $P$ 的概率是正确的。

- **举例：**
    
    - 如果模型预测 100 个样本的概率都是 **80%**，那么这 100 个样本中应该有大约 **80 个**是正确分类的。
        
    - 如果模型预测 100 个样本的概率都是 **50%**，那么这 100 个样本中应该有大约 **50 个**是正确分类的。
        

如果模型在预测概率为 80% 的时候，实际的准确率却只有 65%，我们就说这个模型**过度自信（Over-confident）**，它是**未校准 (Uncalibrated)** 的。

##### 2. ECE 和 MCE 是用来做什么的？

**Expected Calibration Error (ECE)** 和 **Maximum Calibration Error (MCE)** 是最常用的**量化模型校准性**的指标。它们通过比较模型的预测**置信度（Confidence）**和实际的**准确率（Accuracy）**来工作。

为了计算这两个指标，通常需要将模型所有的预测概率划分为 $M$ 个**区间（Bins）**，比如 10 个等宽的区间（0-0.1, 0.1-0.2, ... 0.9-1.0）。

---

###### ECE (Expected Calibration Error) 期望校准误差

- **作用：** 衡量模型**平均**而言的校准误差。
    
- **计算方法：** 它是所有置信度区间（Bin）内，**平均置信度**与**实际准确率**之间差异的**加权平均**。
    
    - 权重通常是每个区间内的样本占总样本的比例。
        
- **解读：** ECE 越低，表示模型的预测概率与实际准确率之间的**平均差距越小**，校准性越好。ECE 可以用来衡量模型的**整体校准质量**。
    

$$\text{ECE} = \sum_{m=1}^{M} \frac{|B_m|}{N} \left| \text{acc}(B_m) - \text{conf}(B_m) \right|$$

_其中 $\text{acc}(B_m)$ 是区间 $B_m$ 的准确率，$\text{conf}(B_m)$ 是平均置信度，$\frac{|B_m|}{N}$ 是该区间的权重。_

---

###### MCE (Maximum Calibration Error) 最大校准误差

- **作用：** 衡量模型校准误差的**最坏情况**。
    
- **计算方法：** 它是所有置信度区间中，**平均置信度**与**实际准确率**之间差异的**最大值**。
    
- **解读：** MCE 越低，表示模型在**任何一个**置信度区间内都没有出现**灾难性的**校准偏差。MCE 对于**高风险应用**（如医疗诊断、自动驾驶）特别重要，因为这些场景需要最大限度地减少最坏情况下的置信度偏差。
    

$$\text{MCE} = \max_{m \in \{1, \dots, M\}} \left| \text{acc}(B_m) - \text{conf}(B_m) \right|$$

---

##### 📊 可视化工具：可靠性图（Reliability Diagram）

为了更好地理解 ECE 和 MCE，我们通常会使用**可靠性图（Reliability Diagram）** 来进行可视化。

- **图的横轴：** 模型的平均预测**置信度**（按区间划分）。
    
- **图的纵轴：** 模型在对应置信度区间内的实际**准确率**。
    
- **理想线：** 一条从 (0, 0) 到 (1, 1) 的对角线。如果模型完美校准，图中的点就应该落在这条线上。
    

**ECE** 衡量的是所有点到对角线距离的**加权平均**，而 **MCE** 衡量的是所有点到对角线距离的**最大值**。

#### ECE (Expected Calibration Error)和 MCE (Maximum Calibration Error)例子
好的，我们来用您给出的 **3 个样本**的例子，演示如何计算 **ECE** 和 **MCE**。

为了计算这两个指标，我们需要将置信度（即模型预测的最高概率）划分到不同的**区间 (Bins)** 中。由于我们只有三个样本，为了简化演示，我们使用 3 个自定义的区间：

| **置信度区间 Bm​**         | **样本 A conf=0.80** | **样本 B conf=0.50** | **样本 C conf=0.55** |
| --------------------- | ------------------ | ------------------ | ------------------ |
| **$B_1$: [0.0, 0.4]** | -                  | -                  | -                  |
| **$B_2$: (0.4, 0.6]** | -                  | 样本 B (0.50)        | 样本 C (0.55)        |
| **$B_3$: (0.6, 1.0]** | 样本 A (0.80)        | -                  | -                  |

总样本数 $N = 3$。

---

##### 步骤一：计算每个区间的指标

我们针对 $B_2$ 和 $B_3$ 这两个非空区间进行计算：

###### 📊 区间 $B_2$: (0.4, 0.6]

1. **平均置信度 $\text{conf}(B_2)$:**
    
    $$\text{conf}(B_2) = \frac{0.50 + 0.55}{2} = \frac{1.05}{2} = \mathbf{0.525}$$
    
2. **实际准确率 $\text{acc}(B_2)$:**
    
    - 样本 B (置信度 0.50): **正确**
        
    - 样本 C (置信度 0.55): 错误
        
        $$\text{acc}(B_2) = \frac{\text{正确样本数}}{|B_2|} = \frac{1}{2} = \mathbf{0.50}$$
        
3. 校准误差 $\text{CalError}(B_2)$:
    
    $$\text{CalError}(B_2) = |\text{acc}(B_2) - \text{conf}(B_2)| = |0.50 - 0.525| = \mathbf{0.025}$$
    

###### 📊 区间 $B_3$: (0.6, 1.0]

1. **平均置信度 $\text{conf}(B_3)$:**
    
    $$\text{conf}(B_3) = \frac{0.80}{1} = \mathbf{0.80}$$
    
2. **实际准确率 $\text{acc}(B_3)$:**
    
    - 样本 A (置信度 0.80): 正确
        
        $$\text{acc}(B_3) = \frac{\text{正确样本数}}{|B_3|} = \frac{1}{1} = \mathbf{1.00}$$
        
3. 校准误差 $\text{CalError}(B_3)$:
    
    $$\text{CalError}(B_3) = |\text{acc}(B_3) - \text{conf}(B_3)| = |1.00 - 0.80| = \mathbf{0.20}$$
    

---

##### 步骤二：计算 ECE (Expected Calibration Error)

ECE 是所有区间校准误差的**加权平均值**。权重是该区间的样本数 $|B_m|$ 占总样本数 $N$ 的比例。

$$\text{ECE} = \sum_{m=1}^{M} \frac{|B_m|}{N} \cdot \text{CalError}(B_m)$$

| 区间 | $\frac{\vert B_m \vert}{N}$ (权重) | $\text{CalError}(B_m)$ | $\text{权重} \times \text{误差}$ |
| :--- | :--- | :--- | :--- |
| $B_2$ | $\frac{2}{3}$ | $0.025$ | $\frac{2}{3} \times 0.025 \approx 0.0167$ |
| $B_3$ | $\frac{1}{3}$ | $0.20$ | $\frac{1}{3} \times 0.20 \approx 0.0667$ |

$$\text{ECE} \approx 0.0167 + 0.0667 = \mathbf{0.0834}$$

##### 步骤三：计算 MCE (Maximum Calibration Error)

MCE 是所有区间的校准误差中的**最大值**。

$$\text{MCE} = \max_{m} (\text{CalError}(B_m))$$

$$\text{MCE} = \max(0.025, 0.20) = \mathbf{0.20}$$

##### 结论

在这个小样本的例子中：

- **ECE $\approx 0.0834$**：表示该模型的**平均**校准误差尚可。
    
- **MCE $= 0.20$**：表示该模型在置信度最高的区间 $B_3$ **(0.6, 1.0]** 表现出最大的不一致。模型在这个区间内的平均置信度是 $0.80$，但实际准确率达到了 $1.00$，说明模型**略微**存在**过度谨慎（Under-confident）**的倾向，即它本可以更自信地做出预测。
    

---

### Hit Rate (Hit@k)

**含义：** 只要前 $k$ 个推荐列表中包含了一个用户感兴趣的物品，就计为 1（命中），否则为 0。它反映了模型“至少能推对一个”的能力。

公式：

$$\text{Hit@k} = \frac{1}{|U|} \sum_{u \in U} I(\text{rel}_u \cap R_{u,k} \neq \emptyset)$$

其中 $I(\cdot)$ 是指示函数，$R_{u,k}$ 是为用户推荐的前 $k$ 个物品集合。

---

### Precision (精度/准确率)

**含义：** 在推荐的前 $k$ 个物品中，有多少比例是用户感兴趣的。

公式：

$$\text{Precision@k} = \frac{|\text{Relevant Items} \cap \text{Top-k Items}|}{k}$$

---

### Recall (召回率)

**含义：** 用户所有感兴趣的物品中，有多少比例被排在了前 $k$ 个推荐位里。

公式：

$$\text{Recall@k} = \frac{|\text{Relevant Items} \cap \text{Top-k Items}|}{|\text{All Relevant Items}|}$$

---

### F1-Score

**含义：** Precision 和 Recall 的调和平均数，用于综合权衡两者的表现。

公式：

$$F1@k = 2 \cdot \frac{\text{Precision@k} \cdot \text{Recall@k}}{\text{Precision@k} + \text{Recall@k}}$$

---

### MRR (Mean Reciprocal Rank，平均倒数排名)

**含义：** 关注**第一个**正确结果出现的位置。如果第一个正确答案排在第 $r$ 位，则分数为 $1/r$。分数越高，说明正确答案排得越靠前。

公式：

$$\text{MRR} = \frac{1}{|U|} \sum_{i=1}^{|U|} \frac{1}{\text{rank}_i}$$

其中 $\text{rank}_i$ 是第 $i$ 个用户第一个命中物品的排名位置。

---

### MAP (Mean Average Precision，平均准确率均值)

**含义：** 考虑了所有正确答案的排名。它先计算每个用户的 Average Precision (AP)，再对所有用户取平均。

公式：

$$\text{AP} = \frac{\sum_{n=1}^k (\text{Precision@n} \times \text{rel}(n))}{\text{Total Relevant Items}}$$

$$\text{MAP} = \frac{1}{|U|} \sum_{u \in U} \text{AP}_u$$

其中 $\text{rel}(n)$ 为第 $n$ 个位置是否相关（1 或 0）。

---

### NDCG (Normalized Discounted Cumulative Gain，归一化折损累计增益)

**含义：** 最常用的排名指标。它不仅关心是否命中，更关心**命中的位置**。位置越靠前，权重（Gain）越高。

- **DCG (折损增益):**
    
    $$\text{DCG}_k = \sum_{i=1}^k \frac{2^{rel_i} - 1}{\log_2(i+1)}$$
    
- **IDCG:** 理想状态下（即把所有相关项都排在最前面）的 DCG。
    
- **NDCG:**
    
    $$\text{NDCG}_k = \frac{\text{DCG}_k}{\text{IDCG}_k}$$
    

> **注意：** `linear` 和 `exponential` 指的是 DCG 公式中对相关度 ($rel_i$) 的处理方式。
> 
> - **Linear:** 直接使用相关度分值。
>     
> - **Exponential:** 使用 $2^{rel_i} - 1$，这会大幅增加高相关度物品排在前面的奖励。
>     

---

### 指标间关系

- Hit@1 = Precision@1 = Recall@1 = MRR@1 = 0.456：
    
    这是因为在 $k=1$ 且每个用户通常只有一个正样本（常见于 Leave-one-out 评估法）时，这些指标在数学上是等价的。
    
- Precision 随 @k 增大而下降：
    
    这是正常的。因为分母 $k$ 在变大，但用户的正样本数量通常很少（可能只有 1 个），所以 $k$ 越大，Precision 越稀释。

MRR v.s. NDCG

- **如果你的任务中每个搜索/推荐只有一个标准答案：** 两者排序基本一致，选哪个都行（MRR 更常用，简单直观）。
    
- **如果你的任务中存在多个正确答案（或者有相关分值）：** 它们的排序**经常会发生分歧**。此时 **NDCG** 被认为是更权威、更精细的指标，因为它奖励了模型召回更多正确项的能力。